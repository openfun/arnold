---
env_type: development  # default to be overriden on command line
customer: eugene  # default to be overriden on command line
project_name: "{{ env_type }}-{{ customer }}"

# Environments
#
# Each environment is defined by a name and a code. These environments can be
# extended given your needs, but they should respect the following conventions:
#
# 1. the current env_type should match one of those environments name,
# 2. an environment code should be unique among all environments.
#
# You should note that the environment code will be used to generate database
# credentials with the following pattern: 
#
# {{ environment.code }}_{{ customer }}_{{ app.name }}
#
# For example, the database name for the marsha application deployed in
# development environment for the eugene customer will be "d_eugene_marsha".
environments:
  - name: "ci"
    code: "c"
  - name: "development"
    code: "d"
  - name: "feature"
    code: "f"
  - name: "production"
    code: "p"
  - name: "staging"
    code: "s"
  - name: "preprod"
    code: "t"

# Openshift applications list for the project
# Example :
# apps:
#   - name: hello
#     services:
#       - name:     hello
#         host:     "{{ project_name }}.{{ domain_name }}"
#         port:     "8080"
#         env:
#           - name:   "RESPONSE"
#             value:  "Hello OpenShift! by Arnold"
#   - name: richie
#     services:
#       - name:     richie
#       - name:     richie-nginx
#         host:     "{{ env_type }}.richie.{{ domain_name }}"
#       - name:     postgresql
#         endpoint: true  # default False

# List environments that are trashable, i.e. not defining services using
# endpoints but deployment configurations for their databases.
trashable_env_types:
  - "ci"
  - "development"
  - "feature"

# Blue/Green deployment route prefixes
blue_green_route_prefixes:
  - "previous"
  - "current"
  - "next"

# Following a blue-green deployment strategy, for each deployment, we create a
# unique identifier (deployment_stamp) that will be used to create unique
# OpenShift object names & tags and connect them. This identifier starts with a
# "d" and contains the deployment datetime (with seconds precision), e.g.
# d-180611-08h46m30s. Default value is null as it should never be used as is,
# but initiated in a playbook using a "set_fact" task.
deployment_stamp: null

# Similarly to deployment stamps, we make job identifiers unique for each
# running playbook (deploy, create_object) by adding the job datetime (with
# seconds precision) to the job name, e.g.
# richie-collecstatic-j-180611-08h46m30s. Default value is null as it should
# never be used as is, but initiated in a playbook using a "set_fact" task.
job_stamp: null

# OpenShift's internal docker registry server
internal_docker_registry: "docker-registry.default.svc:5000"

# ACME (Automated Certificate Management Environment) We use the letsencrypt
# staging environment except in (pre)production (see:
# https://letsencrypt.org/docs/staging-environment)
acme_env: staging
# If the openshift-acme role has been created globally (ClusterRole), we don't
# need to create it for each project. Hence set "has_acme_cluster_role" to
# "true"
has_acme_cluster_role: false

# Expected vault path
vault_path: "group_vars/customer/{{ customer }}/{{ env_type }}/secrets"

# Protect public services behind HTTP basic authentication
activate_http_basic_auth: false
# The message that will be displayed by the client browser
http_basic_auth_message: "Restricted Area"
# htpasswd absolute or relative path (relative to nginx configuration)
http_basic_auth_user_file: "/etc/nginx/conf/htpasswd"
http_basic_auth_users:
  - "admin"
  - "{{ customer }}-admin"

# TODO: move the following settings to the redirect app
# Ports
aliases_port: 8999
nginx_ports:
  - "{{ aliases_port }}"

# Endpoints IP
# These endpoints are used when you don't run your databases inside an OpenShift cluster
# but you have dedicated external resource to manage your databases.
# In trashable environments these endpoints will not be used because everything will
# run inside OpenShift pods. in Production and Pre-production environments you should consider using a dedicated cluster
# for your databases.
# Endpoint IPs are initialized at "null" by default and you SHOULD overwrite them in the corresponding environment where you will use them.
endpoint_mongodb_ip:     null
endpoint_mysql_ip:       null
endpoint_postgresql_ip:  null
