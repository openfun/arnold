---
env_type: development  # default to be overriden on command line
customer: eugene  # default to be overriden on command line
namespace_name: "{{ env_type }}-{{ customer }}"

# Environments
#
# Each environment is defined by a name and a code. These environments can be
# extended given your needs, but they should respect the following conventions:
#
# 1. the current env_type should match one of those environments name,
# 2. an environment code should be unique among all environments.
#
# You should note that the environment code will be used to generate database
# credentials with the following pattern:
#
# {{ environment.code }}_{{ customer }}_{{ app.name }}
#
# For example, the database name for the marsha application deployed in
# development environment for the eugene customer will be "d_eugene_marsha".
environments:
  - name: "ci"
    code: "c"
  - name: "development"
    code: "d"
  - name: "feature"
    code: "f"
  - name: "production"
    code: "p"
  - name: "staging"
    code: "s"
  - name: "preprod"
    code: "t"

# Paths to look for deployable applications
apps_paths:
  - "apps/"

core_apps_paths:
  - "core_apps/"

# List environments that are trashable, i.e. not defining services using
# endpoints but deployment configurations for their databases.
trashable_env_types:
  - "ci"
  - "development"
  - "feature"

# Blue/Green deployment route prefixes
blue_green_prefixes:
  - "previous"
  - "current"
  - "next"

# Following a blue-green deployment strategy, for each deployment, we create a
# unique identifier (deployment_stamp) that will be used to create unique
# OpenShift object names & tags and connect them. This identifier starts with a
# "d" and contains the deployment datetime (with seconds precision), e.g.
# d-180611-08h46m30s. Default value is null as it should never be used as is,
# but initiated in a playbook using a "set_fact" task.
deployment_stamp: null

# Similarly to deployment stamps, we make job identifiers unique for each
# running playbook (deploy, create_object) by adding the job datetime (with
# seconds precision) to the job name, e.g.
# richie-collecstatic-j-180611-08h46m30s. Default value is null as it should
# never be used as is, but initiated in a playbook using a "set_fact" task.
job_stamp: null

# OpenShift's internal docker registry server
internal_docker_registry: "docker-registry.default.svc:5000"

# ACME (Automated Certificate Management Environment)
# We use the letsencrypt staging-environment except in production
# https://letsencrypt.org/docs/staging-environment/
# Set this value to "live" to use the live environment.
acme_env: staging

# (optional) Email address linked to the ACME account.
# It will be used to receive certificate expiration notice.
# Note that one ACME account is created per namespace and
# that multiple accounts can share the same email address.
acme_contact_email: ""

# (required) Name of the cert-manager Issuer entity created
# by arnold for the namespace.
acme_issuer_name: "arnold-acme-issuer-{{acme_env}}"

# Which route prefix should have a certificate generated by acme
acme_enabled_route_prefix:
  - current
  - previous
  - next

# Expected vault path
vault_path: "group_vars/customer/{{ customer }}/{{ env_type }}/secrets"

# Protect public services behind HTTP basic authentication (global setting that
# applies for every compatible app of the project). Note that some apps (e.g.
# edxapp, Richie, Marsha...) can be protected individually by setting the "{{
# app.name }}_activate_http_basic_auth" variable to true.
activate_http_basic_auth: false
# The message that will be displayed by the client browser
http_basic_auth_message: "Restricted Area"
# htpasswd absolute or relative path (relative to nginx configuration)
http_basic_auth_user_file: "/etc/nginx/conf/htpasswd"
http_basic_auth_users:
  - "admin"
  - "{{ customer }}-admin"

# Should we force Route object substitution (delete/create) when it already
# exists?
force_route: false

# Endpoints IP
# These endpoints are used when you don't run your databases inside an OpenShift cluster
# but you have dedicated external resource to manage your databases.
# In trashable environments these endpoints will not be used because everything will
# run inside OpenShift pods. in Production and Pre-production environments you should consider using a dedicated cluster
# for your databases.
# Endpoint IPs are initialized at "null" (except for mongodb we initalize it with an empty list) by default
# and you SHOULD overwrite them in the corresponding environment where you will use them.
endpoint_mongodb_ips:     []
endpoint_mysql_ip:       null
endpoint_postgresql_ip:  null

# Images
# It is recommended to pull images with an authenticated user (in particular, Docker Hub now
# applies stringent rate limits on image pulls by anonymous users).
# The secret name should correspond to the name of a secret added to your project.
# If necessary, the pull secret names can also be configured at the level of each app.
default_image_pull_secret_name: ""


# K8s namespaces
# You can specify extra labels that will be applied to k8s namespaces created by arnold
k8s_namespace_labels:
  managed_by: arnold


# Network policy

# Enable namespace isolation
# If the following parameter is set to true, a network policy will be created
# per namespace. Pods in this namespace will only accept traffic from the ingress
# controller and from pods of the same namespace.
#
# This is disabled by default, because it works out of the box only with
# k3d cluster + ingress-nginx controller. It needs to be configured to
# work on other environments (see network_policy_rule_ingress_controller)
network_policy_per_namespace_enabled: false

# Define the "from" selector to match the ingress controller
# It will be used in the network policy to authorize ingress traffic.
# These default values are working with a k3d cluster + ingress-nginx
# controller setup by the bin/init-cluster script. You should tweak
# these values to match your case.
network_policy_rule_ingress_controller:
  - namespaceSelector:
      matchLabels:
        "app.kubernetes.io/name": ingress-nginx
    podSelector:
      matchLabels:
        "app.kubernetes.io/component": controller
        "app.kubernetes.io/name": ingress-nginx


# UID to use for running containers on k8s.
#
# It will be used in k8s templates to specify the SecurityContext.RunAsUser
# field, to force pods to run with this specific UID.
#
# It prevents from running a container as root.
#
# We recommend that you specify a unique UID per customer/environment.
container_uid: 10000

# GID to use for running containers on k8s.
# This value should be set to 0 (root).
container_gid: 0


# Storage class
# A default storage class is not necessarily defined in a k8s cluster. The following
# variables defines the storage class to use for our PVC, depending on the accessMode
# we are requesting (RWO = ReadWriteOnce, RWX = ReadWriteMany).
# If an arnold app requires a specific storageClass, it should be handled in the app
# directly.
default_storage_class_rwx: "manual"
default_storage_class_rwo: "local-path"
